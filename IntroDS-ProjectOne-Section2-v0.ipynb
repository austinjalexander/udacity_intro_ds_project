{
 "metadata": {
  "name": "",
  "signature": "sha256:b1fffcfb7b3691b96f18a99deb2fc2f81fb6f7e1d42093141cecd6e6ded9fc92"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Analyzing the NYC Subway Dataset\n",
      "#Intro to Data Science: Final Project 1, Part 2  \n",
      "#(Short Questions)\n",
      "##Section 2. Linear Regression\n",
      "Austin J. Alexander\n",
      "***"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Import Directives and Initial DataFrame Creation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy as sp\n",
      "import scipy.stats as st\n",
      "import statsmodels.api as sm\n",
      "import scipy.optimize as op\n",
      "\n",
      "from sklearn import linear_model\n",
      "from sklearn.metrics import r2_score\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "filename = '/Users/excalibur/py/nanodegree/intro_ds/final_project/improved-dataset/turnstile_weather_v2.csv'\n",
      "\n",
      "# import data\n",
      "data = pd.read_csv(filename)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Function for Basic Statistics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def describe_variables(features, X, y):\n",
      "    \n",
      "    for x in features:\n",
      "        size, min_max, mean, var, skew, kurt = st.describe(X[:, features.index(x)])\n",
      "        med = np.median(X[:, features.index(x)])\n",
      "        std = np.std(X[:, features.index(x)])\n",
      "        print \"x{0} ({1}):\\n  min = {2}, max = {3},\\n  mean = {4}, median = {5}, var = {6}, std = {7}\".format(features.index(x), x, min_max[0], min_max[1], np.round(mean, decimals=2), med, np.round(var, decimals=2), np.round(std, decimals=2))\n",
      "        \n",
      "    size, min_max, mean, var, skew, kurt = st.describe(y)\n",
      "    med = np.median(y)\n",
      "    std = np.std(y)\n",
      "    print \"y (ENTRIESn_hourly):\\n  min = {0}, max = {1},\\n  mean = {2}, median = {3}, var = {4}, std = {5}\".format(min_max[0], min_max[1], np.round(mean, decimals=2), med, np.round(var, decimals=2), np.round(std, decimals=2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Extract Relevant Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Shape of data:\" +str(data.shape)\n",
      "N = data.shape[0]\n",
      "print \"N = \" + str(N)\n",
      "print \"0.05 * N = \" + str(0.05 * N)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Shape of data:(42649, 27)\n",
        "N = 42649\n",
        "0.05 * N = 2132.45\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Class for Creating Data Samples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class CreateDataSample:\n",
      "    \n",
      "    quantitative_features = ['hour','day_week','tempi']\n",
      "    categorical_features = ['UNIT'] \n",
      "\n",
      "    def __init__(self,n):\n",
      "        self.n = n\n",
      "        self.simple_random_sample_row_indices = np.random.choice(data['ENTRIESn_hourly'].index.values, size=n, replace=False)\n",
      "        self.sample_df = data.loc[self.simple_random_sample_row_indices]\n",
      "        self.X = self.sample_df[quantitative_features].values\n",
      "        self.y = self.sample_df['ENTRIESn_hourly'].values\n",
      "    \n",
      "    def add_categorical_dummies(self,categorical_features):\n",
      "        C = []\n",
      "        for c in categorical_features:\n",
      "            if categorical_features.index(c) == 0:\n",
      "                C = sm.categorical(self.sample_df[c].values, drop=True)\n",
      "            else:\n",
      "                C = np.hstack((C,sm.categorical(self.sample_df[c].values, drop=True)))\n",
      "\n",
      "        self.X = np.hstack((self.X,C))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Formulas Implemented  \n",
      "(i.e., not included in modules/packages)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Normal Equation (an alternative to gradient descent)\n",
      "\n",
      "Return a minimized $\\theta$, where $\\theta$ is a vector of coefficients for the design matrix $X$, so that the sum of square differences between $\\theta^{T}X^{(i)}$ and $y$ is minimized:  \n",
      "$$\\theta = (X^{T}X)^{-1}X^{T}y = X^{+}y$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Class for Creating Learners"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class learn:\n",
      "    \n",
      "    def __init__(self, sample_sizes, method, test_training_data):\n",
      "        \n",
      "        self.sample_sizes = sample_sizes\n",
      "        self.method = method\n",
      "        self.test_training_data = test_training_data\n",
      "        \n",
      "        self.sample = []\n",
      "        self.X_train = []\n",
      "        self.y_train = []\n",
      "        self.X_test = []\n",
      "        self.y_test = []\n",
      "        \n",
      "        # for OLS\n",
      "        self.model = []\n",
      "        self.results = []\n",
      "\n",
      "    def trial(self, display_results, compare):\n",
      "        for sample_size in self.sample_sizes:\n",
      "            # double the size to create both a training and a testing set\n",
      "            self.sample = CreateDataSample(sample_size*2) \n",
      "            self.sample.add_categorical_dummies(categorical_features)\n",
      "\n",
      "            # training sample size\n",
      "            n = self.sample.X.shape[0]\n",
      "            \n",
      "            # number of features\n",
      "            num_of_features = self.sample.X.shape[1]\n",
      "            \n",
      "            # Feature Scaling\n",
      "            # mean normalization (necessary for minimization not to return NaNs)\n",
      "            x_i_bar = []\n",
      "            s_i = []\n",
      "\n",
      "            for i in np.arange(self.sample.X.shape[1]):\n",
      "                x_i_bar.append(np.mean(self.sample.X[:,i]))\n",
      "                s_i.append(np.std(self.sample.X[:,i]))\n",
      "\n",
      "                self.sample.X[:,i] = np.true_divide((np.subtract(self.sample.X[:,i],x_i_bar[i])),s_i[i])\n",
      "    \n",
      "            # final design matrix\n",
      "            self.sample.X = sm.add_constant(self.sample.X)\n",
      "            \n",
      "            # theta parameters \n",
      "            theta = np.zeros(((num_of_features+1),1))\n",
      "\n",
      "            # randomly selected row indices to gather samples\n",
      "            indices = [i for i in np.arange(n)]\n",
      "            random_indices = np.random.choice(indices, size=(sample_size), replace=False) \n",
      "\n",
      "            # select samples\n",
      "            self.X_train = self.sample.X[random_indices]\n",
      "            self.y_train = self.sample.y[random_indices]\n",
      "\n",
      "            # depending on method, implement method\n",
      "            if self.method == 'OLS':\n",
      "                model = sm.OLS(self.y_train, self.X_train)\n",
      "                results = model.fit()\n",
      "                #results = model.fit_regularized()\n",
      "                self.model = model\n",
      "                self.results = results\n",
      "                \n",
      "            # scikit-learn Linear Regression\n",
      "            elif self.method == 'LinReg':\n",
      "                regr = linear_model.LinearRegression()\n",
      "                regr.fit(self.X_train, self.y_train)\n",
      "            # scikit-learn Ridge Regression\n",
      "            elif self.method == 'Ridge':\n",
      "                clf = Ridge(alpha=1.0)\n",
      "                clf.fit(self.X_train, self.y_train)\n",
      "                \n",
      "            elif self.method == 'BFGS':\n",
      "                # cost function\n",
      "                def J(theta):\n",
      "                    return (1.0/(2*n)) * (((self.X_train.dot(theta)) - self.y_train).T).dot(self.X_train.dot(theta) - self.y_train)\n",
      "                # gradient estimated\n",
      "                solution = op.minimize(J, theta)\n",
      "                theta = solution.x\n",
      "                \n",
      "            elif self.method == 'NormEq':\n",
      "                theta = (np.linalg.pinv(self.X_train).dot(self.y_train))\n",
      "                \n",
      "            elif self.method == 'compare':\n",
      "                # OLS\n",
      "                model = sm.OLS(self.y_train, self.X_train)\n",
      "                results = model.fit()\n",
      "                # LinReg\n",
      "                regr = linear_model.LinearRegression()\n",
      "                regr.fit(self.X_train, self.y_train)\n",
      "                # Ridge\n",
      "                clf = Ridge(alpha=1.0)\n",
      "                clf.fit(self.X_train, self.y_train)\n",
      "                # NormEq\n",
      "                theta = (np.linalg.pinv(self.X_train.T.dot(self.X_train)).dot(self.X_train.T)).dot(self.y_train)\n",
      "                \n",
      "            random_indices = np.random.choice(indices, size=(sample_size), replace=False)\n",
      "\n",
      "            if self.test_training_data == False:\n",
      "                self.X_test = self.sample.X[random_indices]\n",
      "                self.y_test = self.sample.y[random_indices]\n",
      "            else:\n",
      "                self.X_test = self.X_train\n",
      "                self.y_test = self.y_train\n",
      "                \n",
      "            if display_results == True:\n",
      "                print \"\\nn = {0}\".format(n/2)\n",
      "                if self.method == 'OLS':\n",
      "                    print \"r = {0:.2f}\".format(np.sqrt(results.rsquared))\n",
      "                    print \"R^2 = {0:.2f}\".format(results.rsquared)\n",
      "                    \n",
      "                elif self.method == 'LinReg':\n",
      "                    print \"r = {0:.2f}\".format(np.sqrt(regr.score(self.X_test, self.y_test)))\n",
      "                    print \"R^2 = {0:.2f}\".format(regr.score(self.X_test, self.y_test))\n",
      "                elif self.method == 'Ridge':\n",
      "                    print \"r = {0:.2f}\".format(np.sqrt(clf.score(self.X_test, self.y_test)))\n",
      "                    print \"R^2 = {0:.2f}\".format(clf.score(self.X_test, self.y_test))\n",
      "                \n",
      "                # too slow!\n",
      "                elif self.method == 'BFGS':\n",
      "                    y_hat = theta.T.dot(self.X_test)\n",
      "                    print \"y_hat = {0:.2f}\".format(y_hat)\n",
      "                \n",
      "                elif self.method == 'NormEq':\n",
      "                    y_hat = self.X_test.dot(theta.T)\n",
      "                    y_bar = np.mean(self.y_test)\n",
      "                    \n",
      "                    total_variation = np.sum((self.y_test - y_bar)**2)\n",
      "                    unexplained_variation = np.sum((self.y_test - y_hat)**2)\n",
      "                    #explained_variation = np.sum((y_hat - y_bar)**2)\n",
      "                    \n",
      "                    r_squared = (1 - (np.true_divide(unexplained_variation,total_variation)))\n",
      "                    \n",
      "                    print \"r = {0:.2f}\".format(np.sqrt(r_squared))\n",
      "                    print \"R^2 = {0:.2f}\".format(r_squared)\n",
      "                    \n",
      "            if compare == True:\n",
      "                print \"\\nn = {0}\".format(sample_size)\n",
      "                \n",
      "                rsquared_values = {}\n",
      "                r_values = {}\n",
      "                \n",
      "                rsquared_values['ols_R2'] = results.rsquared\n",
      "                r_values['ols_r'] = np.sqrt(rsquared_values['ols_R2'])\n",
      "                \n",
      "                #rsquared_values['lin_reg_R2'] = regr.score(self.X_test, self.y_test)    \n",
      "                #r_values['lin_reg_r'] = np.sqrt(rsquared_values['lin_reg_R2'])\n",
      "                \n",
      "                rsquared_values['ridge_R2'] = clf.score(self.X_test, self.y_test)   \n",
      "                r_values['ridge_r'] = np.sqrt(rsquared_values['ridge_R2'])\n",
      "            \n",
      "                # normal equation\n",
      "                y_hat = self.X_test.dot(theta.T)\n",
      "                y_bar = np.mean(self.y_test)\n",
      "                total_variation = np.sum((self.y_test - y_bar)**2)\n",
      "                unexplained_variation = np.sum((self.y_test - y_hat)**2)\n",
      "                    \n",
      "                rsquared_values['norm_eq_R2'] = (1 - (np.true_divide(unexplained_variation,total_variation)))\n",
      "                r_values['norm_eq_r'] = np.mean(rsquared_values['norm_eq_R2'])\n",
      "                \n",
      "                max_rsquared = max(rsquared_values.iteritems(), key=operator.itemgetter(1))\n",
      "                max_r = max(r_values.iteritems(), key=operator.itemgetter(1))\n",
      "                \n",
      "                print \"{0} = {1:.2f}\".format(max_r[0], max_r[1])\n",
      "                print \"{0} = {1:.2f}\".format(max_rsquared[0], max_rsquared[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Section 1. Linear Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3 id='2_1'>2.1 What approach did you use to compute the coefficients theta and produce prediction for ENTRIESn_hourly in your regression model?</h3>\n",
      "\n",
      "After comparing a few different methods (Ordinary Least Squares [OLS] from *StatsModels*, two different regression techniques from *scikit-learn*, the Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno [BFGS] optimization algorithm from *Scipy.optimize*, and a Normal Equations algebraic attempt), OLS from *StatsModels* was chosen due to its consistently higher $r$ and $R^{2}$ values (see notes 1 and 2 below) throughout various test sample sizes ( $n = \\{30, 100, 500, 1500, 5000, 10000\\}$ ).\n",
      "\n",
      "#####Notes\n",
      "<sup>1</sup> The linear correlation coefficient ($r$) can take on the following values: $-1 \\leq r \\leq 1$. If $r = +1$, then a perfect positive linear relation exists between the explanatory and response variables. If $r = -1$, then a perfect negative linear relation exists between the explanatory and response variables.\n",
      "\n",
      "<sup>2</sup> The coefficient of determination ($R^{2}$) can take on the following values: $0 \\leq R^{2} \\leq 1$. If $R^{2} = 0$, the least-squares regression line has no explanatory value; if $R^{2} = 1$, the least-squares regression line explains $100\\%$ of the variation in the response variable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sample-size values are halved \n",
      "sample_sizes = [30, 100, 500, 1500, 5000, 10000]\n",
      "compare = learn(sample_sizes, method='compare', test_training_data=False)\n",
      "compare.trial(display_results=False, compare=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "global name 'quantitative_features' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-6-dd8e3388388a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcompare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'compare'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_training_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcompare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-5-6b7055db02b7>\u001b[0m in \u001b[0;36mtrial\u001b[0;34m(self, display_results, compare)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# double the size to create both a training and a testing set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCreateDataSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_categorical_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-4-417a2d933ac6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_random_sample_row_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ENTRIESn_hourly'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_random_sample_row_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquantitative_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ENTRIESn_hourly'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: global name 'quantitative_features' is not defined"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3 id='2_2'>2.2 What features (input variables) did you use in your model? Did you use any dummy variables as part of your features?</h3>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Quantitative features used: 'hour','day_week','rain','tempi'.\n",
      "\n",
      "Categorical features used: 'UNIT'. As a categorical feature, this variable required the use of dummy variables."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3 id='2_3'>2.3 Why did you select these features in your model?</h3>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Due to the findings presented in the <a href='IntroDS-ProjectOne-DataExploration-Supplement.ipynb' target='_blank'>*DataExploration* supplement</a>, it seemed clear that location significantly impacted the number of entries. In addition, the hour and day of the week showed importance. Temperature appeared to have some relationship with entries as well, and so it was included. Based on that exploration and on the statistical and practical evidence offered in <a href='IntroDS-ProjectOne-Section1.ipynb' target='_blank'>Section 1. Statistical Test</a>, rain was not included as a feature (and, as evidenced by a number of test runs, had marginal if any importance).\n",
      "\n",
      "\n",
      "As far as the selection of location and day/time variables were concerned, **station** can be captured quantitatively by **latitude** and **longitude**, both of which, as numeric values, should offer a better sense of trend toward something. However, as witnessed by numerous test runs, **latitude** and **longitude** in fact appear to be redundant when using **UNIT** as a feature, which is in fact more signficant (as test runs indicated and, as one might assume, due to, for example, station layouts) than **latitude** and **longitude**.\n",
      "\n",
      "Each **DATEn** is a 'one-off', so it's unclear how any could be helpful for modeling/predicting (as those dates literally never occur again). **day_week** seemed to be a better selection in this case."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Using StatsModels OLS to Create a Model (100 trials)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sample_size = [500]\n",
      "ols = []\n",
      "\n",
      "for i in np.arange(100):\n",
      "    temp_ols = learn(sample_size, method='OLS', test_training_data=False)\n",
      "    temp_ols.trial(display_results=False, compare=False)\n",
      "    if not ols or (temp_ols.results.rsquared > ols.results.rsquared):\n",
      "        ols = temp_ols\n",
      "        \n",
      "print \"\\nn = {0}\".format(sample_size[0])\n",
      "print \"r = {0:.2f}\".format(np.sqrt(ols.results.rsquared))\n",
      "print \"R^2 = {0:.2f}\".format(ols.results.rsquared)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "observed = ols.y_test\n",
      "predictions = ols.results.predict(ols.X_test)\n",
      "plt.title('Observed Values vs Fitted Predictions')\n",
      "plt.xlabel('observed values')\n",
      "plt.ylabel('predictions')\n",
      "plt.scatter(observed, predictions, alpha=0.7, color='green', edgecolors='black')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3 id='2_4'>2.4 What are the coefficients (or weights) of the features in your linear regression model?</h3>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ols.results.params"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3 id='2_5'>2.5 What is your model\u2019s $R^{2}$ (coefficient of determination) value?</h3>\n",
      "\n",
      "For $n = 500$, the best $R^{2}$ value witnessed was $0.85$ (with the best $r$ value seen at $0.92$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3 id='2_6_a'>2.6.a What does this $R^{2}$ value mean for the goodness of fit for your regression model?</h3>\n",
      "\n",
      "This $R^{2}$ value means that $85\\%$ of the proportion of total variation in the response variable is explained by the least-squares regression line (i.e., model) that was created above."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3 id='2_6_b'>2.6.b Do you think this linear model to predict ridership is appropriate for this dataset, given this $R^{2}$ value?</h3>\n",
      "\n",
      "It's better than guessing in the dark, but too much shouldn't be staked on its predictions:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Predictions and their Residual Differences from Observed Values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_hat_test = ols.results.predict(ols.X_test)\n",
      "residuals = (y_hat_test - ols.y_test)\n",
      "y = pd.DataFrame(data={'y_test':ols.y_test,'y_hat_test':y_hat_test,'residuals':residuals}, columns=['y_test','y_hat_test','residuals'])\n",
      "y['residuals'] = residuals\n",
      "\n",
      "entries_diff = [1, 100, 1000]\n",
      "\n",
      "for i in entries_diff:\n",
      "    proximity = ['close' if d < i else 'far' for d in residuals]\n",
      "    y['proximity'] = proximity\n",
      "\n",
      "    total = y['proximity'].count()\n",
      "    close = y[y['proximity'] == 'close']['proximity'].count()\n",
      "\n",
      "    print \"residuals < {0} ( {1}% )\".format(i, np.true_divide(close,total)*100)\n",
      "    print y['proximity'].describe()\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As can be seen from the above, somewhat arbitrarily-selected, values, the number of *close* predictions is a little over $50\\%$ when *close* is defined as a prediction with a difference that is less than $1$ from the actual observed value. Given that the value of entries can take on such a large range of values $[0, 32814]$, differences less than $100$ and $1000$ are shown as well. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Residual Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.boxplot(residuals, vert=False)\n",
      "plt.title('Boxplot of Residuals')\n",
      "plt.xlabel('residuals')\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(y_hat_test,residuals, alpha=0.7, color='green', edgecolors='black')\n",
      "plt.title('RESIDUAL PLOT')\n",
      "plt.plot([np.min(y_hat_test),np.max(y_hat_test)], [0, 0], color='red')\n",
      "plt.xlabel('predictions')\n",
      "plt.ylabel('residuals')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since the above predictions show a discernible, linear, and increasing pattern (and, thus, are not stochastic), it seems apparent that there is in fact not a linear relationship between the explanatory and response variables. Thus, a linear model is not appropriate for the current data set."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}